---
layout: page
title: HPC facilities at Exeter
order: 3
session: 1
length: 20
toc: true
---

## Questions

    What HPC facilities are available at Exeter?

## Objectives

    Describe the HPC infrastructure at Exeter.

    Learn how to gain access to HPC systems at Exeter.

The University of Exeter has it's own central HPC facility to enable intensive computational research. It represents a Â£3m investment by the University, designed to serve the advanced computing requirements of all research disciplines.  ISCA is available free of charge to all research groups on all campuses. All research active staff are able to access ISCA, from any University of Exeter campus or via a VPN connection. Once access has been granted you can connect using your existing Exter IT account.

## ISCA specifications

The first of its kind in a UK University, ISCA combines a traditional HPC cluster with a virtualised cluster environment, providing a range of node types in a single machine. ISCA consists of a range of compute resources: the traditional cluster (128 GB nodes) is complimented by two large memory (3TB) nodes, Xeon Phi accelerator nodes and GPU (Tesla K80) compute nodes. The non-traditional element of ISCA includes a cluster of higher memory nodes (256 GB), 3 TB nodes, and an Openstack environment for the management of virtualised resources. ISCA runs on CentOS 7 Linux.



## OpenStack

## Working with Projects

One way ISCA might be different to other HPC systems you've used is that to run a job on the system you need to be a member of a project. Projects are generally aligned to grants, the grant PI is typically the admin of the project and can manage the addition of new members. They can also add other admins. 

Each project has a unique ID, that you will need to specify when submitting a compute job to the system. Projects also come with shared storage. We will look at identifying which projects you are a member of once we have logged in shortly.

## Fair Usage

There are major benefits in terms of capacity and capability to using a central HPC system. There are also logistical and practical challenges that arise when using a shared resource. There are two elements of the system that are shared, the storage and compute. Schedulers deal with the allocation and protection of compute, and we will cover these in more detail later on. Storage might be managed by applying file quotas to user folders. It is likely there is also a fair usage policy in action designed to ensure everyone can get access to a compute node within a reasonable period of time. Essentially, whereas on your computer you can use it how you want, when you want with no imapct of others, with a HPC system, you may need to be patient at times and be considerate of other users. 



## Citing use of ISCA

Please use the following when acknowledging the use of ISCA in research papers:

```
The authors would like to acknowledge the use of the University of Exeter High-Performance Computing (HPC) facility in carrying out this work.
```
Also, when you submit your paper or article to Symplectic please add ARC - ISCA as an Unclassified Label.

## Associated resources
